{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian network 5th question"
      ],
      "metadata": {
        "id": "mRBxdI7_Ro6v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpNpEHtVEjUO",
        "outputId": "be974b82-dde3-4fe2-f1e3-eb6b7b7d250e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of computer failure given evidence: Yes    0.5\n",
            "No     0.5\n",
            "Name: Computer Failure, dtype: float64\n",
            "Predicted Values:\n",
            "['Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No']\n",
            "Accuracy: 71.42857142857143%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def create_bayesian_network(data):\n",
        "    # Calculate probabilities for each variable\n",
        "    probabilities = {}\n",
        "    variables = data.columns\n",
        "\n",
        "    for variable in variables:\n",
        "        probabilities[variable] = data[variable].value_counts(normalize=True)\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def query_probability(bayesian_network, variable, evidence):\n",
        "    evidence_columns = list(evidence.keys())\n",
        "    evidence_values = list(evidence.values())\n",
        "    filtered_data = data.copy()\n",
        "\n",
        "    # Filter the dataset based on evidence\n",
        "    for column, value in zip(evidence_columns, evidence_values):\n",
        "        filtered_data = filtered_data[filtered_data[column] == value]\n",
        "\n",
        "    # Calculate the probability distribution of the variable given the evidence\n",
        "    probabilities = filtered_data[variable].value_counts(normalize=True)\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.DataFrame({\n",
        "    'Electricity': ['Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No'],\n",
        "    'Computer Malfunction': ['Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'No'],\n",
        "    'Computer Failure': ['Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No']\n",
        "})\n",
        "\n",
        "# Create the Bayesian network\n",
        "bayesian_network = create_bayesian_network(data)\n",
        "\n",
        "# Query probability: Probability of computer failure given evidence\n",
        "evidence = {'Electricity': 'Yes', 'Computer Malfunction': 'Yes'}\n",
        "q = query_probability(bayesian_network, 'Computer Failure', evidence)\n",
        "print(f\"Probability of computer failure given evidence: {q}\")\n",
        "\n",
        "# Make predictions based on evidence\n",
        "predicted_values = []\n",
        "for i in range(len(data)):\n",
        "    instance = data.iloc[i]\n",
        "    evidence = {'Electricity': instance['Electricity'], 'Computer Malfunction': instance['Computer Malfunction']}\n",
        "    q = query_probability(bayesian_network, 'Computer Failure', evidence)\n",
        "    predicted_values.append(q.idxmax())\n",
        "\n",
        "print(\"Predicted Values:\")\n",
        "print(predicted_values)\n",
        "\n",
        "# Calculate accuracy (assuming you have actual values)\n",
        "actual_values = data['Computer Failure']\n",
        "accuracy = np.sum(predicted_values == actual_values) / len(actual_values)\n",
        "print(f\"Accuracy: {accuracy * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_class_probabilities(data):\n",
        "    class_counts = data['Computer Failure'].value_counts()\n",
        "    total_instances = len(data)\n",
        "    class_probabilities = {}\n",
        "\n",
        "    for class_value, count in class_counts.items():\n",
        "        class_probabilities[class_value] = count / total_instances\n",
        "\n",
        "    return class_probabilities\n",
        "\n",
        "def calculate_feature_probabilities(data):\n",
        "    feature_probabilities = {}\n",
        "\n",
        "    for feature in data.columns[:-1]:\n",
        "        feature_probabilities[feature] = {}\n",
        "\n",
        "        for feature_value in data[feature].unique():\n",
        "            feature_counts = data[data[feature] == feature_value]['Computer Failure'].value_counts()\n",
        "            feature_total = feature_counts.sum()\n",
        "\n",
        "            probabilities = {}\n",
        "\n",
        "            for class_value, count in feature_counts.items():\n",
        "                probabilities[class_value] = count / feature_total\n",
        "\n",
        "            feature_probabilities[feature][feature_value] = probabilities\n",
        "\n",
        "    return feature_probabilities\n",
        "\n",
        "def predict(data, class_probabilities, feature_probabilities):\n",
        "    predicted_values = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        instance = data.iloc[i]\n",
        "        probabilities = {}\n",
        "\n",
        "        for class_value, class_probability in class_probabilities.items():\n",
        "            probability = class_probability\n",
        "\n",
        "            for feature in data.columns[:-1]:\n",
        "                feature_value = instance[feature]\n",
        "                feature_probability = feature_probabilities[feature][feature_value][class_value]\n",
        "                probability *= feature_probability\n",
        "\n",
        "            probabilities[class_value] = probability\n",
        "\n",
        "        predicted_values.append(max(probabilities, key=probabilities.get))\n",
        "\n",
        "    return predicted_values\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.DataFrame({\n",
        "    'Electricity': ['Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No'],\n",
        "    'Computer Malfunction': ['Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'No'],\n",
        "    'Computer Failure': ['Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No']\n",
        "})\n",
        "\n",
        "# Calculate class probabilities\n",
        "class_probabilities = calculate_class_probabilities(data)\n",
        "\n",
        "# Calculate feature probabilities\n",
        "feature_probabilities = calculate_feature_probabilities(data)\n",
        "\n",
        "# Make predictions\n",
        "predicted_values = predict(data, class_probabilities, feature_probabilities)\n",
        "\n",
        "print(\"Predicted Values:\")\n",
        "print(predicted_values)\n",
        "\n",
        "# Calculate accuracy (assuming you have actual values)\n",
        "actual_values = data['Computer Failure']\n",
        "accuracy = np.sum(predicted_values == actual_values) / len(actual_values)\n",
        "print(f\"Accuracy: {accuracy * 100}%\")"
      ],
      "metadata": {
        "id": "Ez143XCRT6Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENDING ......................\n"
      ],
      "metadata": {
        "id": "bQeaSyxXVcgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 ND JAPAN DATSET\n",
        "\n"
      ],
      "metadata": {
        "id": "LYxNgN2gNxI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "path = \"/content/Untitled spreadsheet - Sheet1.csv\"\n",
        "data = pd.read_csv(path)\n",
        "print(data, \"\\n\")\n",
        "\n",
        "d = np.array(data)[:, :-1]\n",
        "print(\"\\nThe attributes are:\", d)\n",
        "\n",
        "target = np.array(data)[:, -1]\n",
        "print(\"\\nThe target is:\", target)\n",
        "\n",
        "\n",
        "def findS(c, t):\n",
        "    for i, val in enumerate(t):\n",
        "        if val == \"Positive\":\n",
        "            specifics_hypothesis = c[i].copy()\n",
        "            break\n",
        "\n",
        "    for i, val in enumerate(c):\n",
        "        if t[i] == \"Positive\":\n",
        "            for x in range(len(specifics_hypothesis)):\n",
        "                if val[x] != specifics_hypothesis[x]:\n",
        "                    specifics_hypothesis[x] = '?'\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "    return specifics_hypothesis\n",
        "\n",
        "\n",
        "\n",
        "hypothesis = findS(d, target)\n",
        "print(\"\\nThe final hypothesis is:\", hypothesis)\n",
        "\n",
        "\n",
        "def predict(data, hypothesis):\n",
        "    predicted_targets = []\n",
        "    for instance in data:\n",
        "        match = True\n",
        "        for i in range(len(hypothesis)):\n",
        "            if hypothesis[i] != '?' and hypothesis[i] != instance[i]:\n",
        "                match = False\n",
        "                break\n",
        "        if match:\n",
        "            predicted_targets.append(\"Positive\")\n",
        "        else:\n",
        "            predicted_targets.append(\"Negative\")\n",
        "    return predicted_targets\n",
        "\n",
        "\n",
        "predicted_targets = predict(d, hypothesis)\n",
        "print(\"\\nPredicted targets:\", predicted_targets)\n",
        "\n",
        "\n",
        "def calculate_accuracy(actual, predicted):\n",
        "    correct = sum(1 for a, p in zip(actual, predicted) if a == p)\n",
        "    total = len(actual)\n",
        "    accuracy = correct / total * 100\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "accuracy = calculate_accuracy(target, predicted_targets)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1afPmdqN0_B",
        "outputId": "11ba3575-604d-4041-cdc9-b4498c201ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Origin Manufacturer  Color  Decade     Type Example Type\n",
            "0  Japan        Honda   Blue    1980  Economy     Positive\n",
            "1  Japan       Toyoto  Green    1970   Sports     Negative\n",
            "2  Japan       Toyoto   Blue    1990  Economy     Positive\n",
            "3    USA     Chrysler    Red    1980  Economy     Negative\n",
            "4  Japan        Honda  White    1980  Economy     Positive\n",
            "5  Japan       Toyoto  Green    1980  Economy     Positive \n",
            "\n",
            "\n",
            "The attributes are: [['Japan' 'Honda' 'Blue' 1980 'Economy']\n",
            " ['Japan' 'Toyoto' 'Green' 1970 'Sports']\n",
            " ['Japan' 'Toyoto' 'Blue' 1990 'Economy']\n",
            " ['USA' 'Chrysler' 'Red' 1980 'Economy']\n",
            " ['Japan' 'Honda' 'White' 1980 'Economy']\n",
            " ['Japan' 'Toyoto' 'Green' 1980 'Economy']]\n",
            "\n",
            "The target is: ['Positive' 'Negative' 'Positive' 'Negative' 'Positive' 'Positive']\n",
            "\n",
            "The final hypothesis is: ['Japan' '?' '?' '?' 'Economy']\n",
            "\n",
            "Predicted targets: ['Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive']\n",
            "\n",
            "Accuracy: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "\n",
        "\n",
        "data = pd.DataFrame(data=pd.read_csv('/content/Untitled spreadsheet - Sheet1.csv'))\n",
        "print(data,\"\\n\")\n",
        "\n",
        "concepts = np.array(data.iloc[:,0:-1])\n",
        "print(\"The attributes are: \",concepts)\n",
        "target = np.array(data.iloc[:,-1])\n",
        "print(\"\\nThe target is: \",target)\n",
        "\n",
        "with open(\"/content/Untitled spreadsheet - Sheet1.csv\") as f:\n",
        "    csv_file = csv.reader(f)\n",
        "    data = list(csv_file)\n",
        "\n",
        "    specific = data[1][:-1]\n",
        "    general = [['?' for i in range(len(specific))] for j in range(len(specific))]\n",
        "\n",
        "    for i in data:\n",
        "        if i[-1] == \"Positive\":\n",
        "            for j in range(len(specific)):\n",
        "                if i[j] != specific[j]:\n",
        "                    specific[j] = \"?\"\n",
        "                    general[j][j] = \"?\"\n",
        "\n",
        "        elif i[-1] == \"Negative\":\n",
        "            for j in range(len(specific)):\n",
        "                if i[j] != specific[j]:\n",
        "                    general[j][j] = specific[j]\n",
        "                else:\n",
        "                    general[j][j] = \"?\"\n",
        "\n",
        "        print(\"\\nStep \" + str(data.index(i)) + \" of Candidate Elimination Algorithm\")\n",
        "        print(specific)\n",
        "        print(general)\n",
        "\n",
        "    gh = []  # gh = general Hypothesis\n",
        "    for i in general:\n",
        "        for j in i:\n",
        "            if j != '?':\n",
        "                gh.append(i)\n",
        "                break\n",
        "    print(\"\\nFinal Specific hypothesis:\\n\", specific)\n",
        "    print(\"\\nFinal General hypothesis:\\n\", gh)\n",
        "\n",
        "    def predict(instance, specific_hypothesis, general_hypotheses):\n",
        "        for i in range(len(specific_hypothesis)):\n",
        "            if specific_hypothesis[i] != '?' and specific_hypothesis[i] != instance[i]:\n",
        "                return \"Negative\"\n",
        "\n",
        "        for gh in general_hypotheses:\n",
        "            match = True\n",
        "            for i in range(len(gh)):\n",
        "                if gh[i] != '?' and gh[i] != instance[i]:\n",
        "                    match = False\n",
        "                    break\n",
        "            if match:\n",
        "                return \"Positive\"\n",
        "\n",
        "        return \"Unknown\"\n",
        "\n",
        "    predicted_targets = []\n",
        "    for instance in concepts:\n",
        "        prediction = predict(instance, specific, gh)\n",
        "        predicted_targets.append(prediction)\n",
        "\n",
        "    def calculate_accuracy(actual, predicted):\n",
        "        correct = sum(1 for a, p in zip(actual, predicted) if a == p)\n",
        "        total = len(actual)\n",
        "        accuracy = correct / total * 100\n",
        "        return accuracy\n",
        "\n",
        "    accuracy = calculate_accuracy(target, predicted_targets)\n",
        "    print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukDPubfuOMBY",
        "outputId": "c03626c7-7f18-4ee6-c8fc-da2aa0697a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Origin Manufacturer  Color  Decade     Type Example Type\n",
            "0  Japan        Honda   Blue    1980  Economy     Positive\n",
            "1  Japan       Toyoto  Green    1970   Sports     Negative\n",
            "2  Japan       Toyoto   Blue    1990  Economy     Positive\n",
            "3    USA     Chrysler    Red    1980  Economy     Negative\n",
            "4  Japan        Honda  White    1980  Economy     Positive\n",
            "5  Japan       Toyoto  Green    1980  Economy     Positive \n",
            "\n",
            "The attributes are:  [['Japan' 'Honda' 'Blue' 1980 'Economy']\n",
            " ['Japan' 'Toyoto' 'Green' 1970 'Sports']\n",
            " ['Japan' 'Toyoto' 'Blue' 1990 'Economy']\n",
            " ['USA' 'Chrysler' 'Red' 1980 'Economy']\n",
            " ['Japan' 'Honda' 'White' 1980 'Economy']\n",
            " ['Japan' 'Toyoto' 'Green' 1980 'Economy']]\n",
            "\n",
            "The target is:  ['Positive' 'Negative' 'Positive' 'Negative' 'Positive' 'Positive']\n",
            "\n",
            "Step 0 of Candidate Elimination Algorithm\n",
            "['Japan', 'Honda', 'Blue', '1980', 'Economy']\n",
            "[['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?']]\n",
            "\n",
            "Step 1 of Candidate Elimination Algorithm\n",
            "['Japan', 'Honda', 'Blue', '1980', 'Economy']\n",
            "[['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?']]\n",
            "\n",
            "Step 2 of Candidate Elimination Algorithm\n",
            "['Japan', 'Honda', 'Blue', '1980', 'Economy']\n",
            "[['?', '?', '?', '?', '?'], ['?', 'Honda', '?', '?', '?'], ['?', '?', 'Blue', '?', '?'], ['?', '?', '?', '1980', '?'], ['?', '?', '?', '?', 'Economy']]\n",
            "\n",
            "Step 3 of Candidate Elimination Algorithm\n",
            "['Japan', '?', 'Blue', '?', 'Economy']\n",
            "[['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', 'Blue', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', 'Economy']]\n",
            "\n",
            "Step 4 of Candidate Elimination Algorithm\n",
            "['Japan', '?', 'Blue', '?', 'Economy']\n",
            "[['Japan', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', 'Blue', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?']]\n",
            "\n",
            "Step 5 of Candidate Elimination Algorithm\n",
            "['Japan', '?', '?', '?', 'Economy']\n",
            "[['Japan', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?']]\n",
            "\n",
            "Step 6 of Candidate Elimination Algorithm\n",
            "['Japan', '?', '?', '?', 'Economy']\n",
            "[['Japan', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?']]\n",
            "\n",
            "Final Specific hypothesis:\n",
            " ['Japan', '?', '?', '?', 'Economy']\n",
            "\n",
            "Final General hypothesis:\n",
            " [['Japan', '?', '?', '?', '?']]\n",
            "\n",
            "Accuracy: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 RD QUESTION ID3, NAIVE BAYES\n"
      ],
      "metadata": {
        "id": "3bNlWh0xRPUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import csv\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "def load_csv(filename):\n",
        "    lines = csv.reader(open(filename, \"r\"))\n",
        "    dataset = list(lines)\n",
        "    headers = dataset.pop(0)\n",
        "    return dataset, headers\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, attribute):\n",
        "        self.attribute = attribute\n",
        "        self.children = []\n",
        "        self.answer = \"\"\n",
        "\n",
        "def subtables(data, col, delete):\n",
        "    dic = {}\n",
        "    coldata = [row[col] for row in data]\n",
        "    attr = list(set(coldata))\n",
        "\n",
        "    counts = [0] * len(attr)\n",
        "    r = len(data)\n",
        "    c = len(data[0])\n",
        "    for x in range(len(attr)):\n",
        "        for y in range(r):\n",
        "            if data[y][col] == attr[x]:\n",
        "                counts[x] += 1\n",
        "\n",
        "    for x in range(len(attr)):\n",
        "        dic[attr[x]] = [[0 for i in range(c)] for j in range(counts[x])]\n",
        "        pos = 0\n",
        "        for y in range(r):\n",
        "            if data[y][col] == attr[x]:\n",
        "                if delete:\n",
        "                    del data[y][col]\n",
        "                dic[attr[x]][pos] = data[y]\n",
        "                pos += 1\n",
        "    return attr, dic\n",
        "\n",
        "def entropy(S):\n",
        "    attr = list(set(S))\n",
        "    if len(attr) == 1:\n",
        "        return 0\n",
        "\n",
        "    counts = [0, 0]\n",
        "    for i in range(2):\n",
        "        counts[i] = sum([1 for x in S if attr[i] == x]) / (len(S) * 1.0)\n",
        "\n",
        "    sums = 0\n",
        "    for cnt in counts:\n",
        "        sums += -1 * cnt * math.log(cnt, 2)\n",
        "    return sums\n",
        "\n",
        "def compute_gain(data, col):\n",
        "    attr, dic = subtables(data, col, delete=False)\n",
        "\n",
        "    total_size = len(data)\n",
        "    entropies = [0] * len(attr)\n",
        "    ratio = [0] * len(attr)\n",
        "\n",
        "    total_entropy = entropy([row[-1] for row in data])\n",
        "    for x in range(len(attr)):\n",
        "        ratio[x] = len(dic[attr[x]]) / (total_size * 1.0)\n",
        "        entropies[x] = entropy([row[-1] for row in dic[attr[x]]])\n",
        "        total_entropy -= ratio[x] * entropies[x]\n",
        "    return total_entropy\n",
        "\n",
        "def build_tree(data, features):\n",
        "    lastcol = [row[-1] for row in data]\n",
        "    if len(set(lastcol)) == 1:\n",
        "        node = Node(\"\")\n",
        "        node.answer = lastcol[0]\n",
        "        return node\n",
        "\n",
        "    n = len(data[0]) - 1\n",
        "    gains = [0] * n\n",
        "    for col in range(n):\n",
        "        gains[col] = compute_gain(data, col)\n",
        "    split = gains.index(max(gains))\n",
        "    node = Node(features[split])\n",
        "    fea = features[:split] + features[split + 1:]\n",
        "\n",
        "    attr, dic = subtables(data, split, delete=True)\n",
        "\n",
        "    for x in range(len(attr)):\n",
        "        child = build_tree(dic[attr[x]], fea)\n",
        "        node.children.append((attr[x], child))\n",
        "    return node\n",
        "\n",
        "def print_tree(node, level):\n",
        "    if node.answer != \"\":\n",
        "        print(\"  \" * level, node.answer)\n",
        "        return\n",
        "\n",
        "    print(\"  \" * level, node.attribute)\n",
        "    for value, n in node.children:\n",
        "        print(\"  \" * (level + 1), value)\n",
        "        print_tree(n, level + 2)\n",
        "\n",
        "def classify(node, x_test, features):\n",
        "    if node.answer != \"\":\n",
        "        return node.answer\n",
        "    pos = features.index(node.attribute)\n",
        "    for value, n in node.children:\n",
        "        if x_test[pos] == value:\n",
        "            return classify(n, x_test, features)\n",
        "\n",
        "'''Main program'''\n",
        "dataset, features = load_csv(\"/content/climate_exam (1).csv\")\n",
        "node1 = build_tree(dataset, features)\n",
        "\n",
        "print(\"The decision tree for the dataset using ID3 algorithm is\")\n",
        "print_tree(node1, 0)\n",
        "\n",
        "testdata, features = load_csv(\"/content/climate_exam (1).csv\")\n",
        "\n",
        "test_instances = [instance[:-1] for instance in testdata]\n",
        "true_labels = [instance[-1] for instance in testdata]\n",
        "\n",
        "predicted_labels = []\n",
        "for xtest in test_instances:\n",
        "    predicted_label = classify(node1, xtest, features)\n",
        "    predicted_labels.append(predicted_label)\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKnVsb41RcMq",
        "outputId": "b3a2b54e-67db-4fcf-e029-5d8e8b98be86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decision tree for the dataset using ID3 algorithm is\n",
            " Parents\n",
            "   Yes\n",
            "     Cinema\n",
            "   No\n",
            "     Weather\n",
            "       Sunny\n",
            "         Tennis\n",
            "       Windy\n",
            "         Money\n",
            "           Rich\n",
            "             Shopping\n",
            "           Poor\n",
            "             Cinema\n",
            "       Rainy\n",
            "         Stay in\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "path = \"/content/climate_exam (1).csv\"\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "# Print the initial dataset\n",
        "print(\"Initial Dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
        "\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.classes = None\n",
        "        self.class_probabilities = None\n",
        "        self.feature_probabilities = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.class_probabilities = self.calculate_class_probabilities(y)\n",
        "        self.feature_probabilities = self.calculate_feature_probabilities(X, y)\n",
        "\n",
        "    def calculate_class_probabilities(self, y):\n",
        "        class_probabilities = {}\n",
        "        total_samples = len(y)\n",
        "        for class_label in self.classes:\n",
        "            class_samples = np.sum(y == class_label)\n",
        "            class_probabilities[class_label] = class_samples / total_samples\n",
        "        return class_probabilities\n",
        "\n",
        "    def calculate_feature_probabilities(self, X, y):\n",
        "        feature_probabilities = {}\n",
        "        for class_label in self.classes:\n",
        "            class_samples = X[y == class_label]\n",
        "            feature_probabilities[class_label] = {\n",
        "                feature_index: self.calculate_probability(class_samples[:, feature_index])\n",
        "                for feature_index in range(X.shape[1])\n",
        "            }\n",
        "        return feature_probabilities\n",
        "\n",
        "    def calculate_probability(self, feature_values):\n",
        "        total_samples = len(feature_values)\n",
        "        value_counts = np.unique(feature_values, return_counts=True)\n",
        "        probabilities = dict(zip(value_counts[0], value_counts[1] / total_samples))\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for instance in X_test:\n",
        "            class_scores = []\n",
        "            for class_label in self.classes:\n",
        "                class_score = self.class_probabilities[class_label]\n",
        "                for feature_index, feature_value in enumerate(instance):\n",
        "                    feature_probability = self.feature_probabilities[class_label][feature_index]\n",
        "                    if feature_value in feature_probability:\n",
        "                        class_score *= feature_probability[feature_value]\n",
        "                    else:\n",
        "                        class_score *= 0  # Handle case when feature value is not present in probability dictionary\n",
        "                class_scores.append(class_score)\n",
        "            predicted_class = self.classes[np.argmax(class_scores)]\n",
        "            predictions.append(predicted_class)\n",
        "        return predictions\n",
        "\n",
        "    def score(self, X_test, y_test):\n",
        "        predictions = self.predict(X_test)\n",
        "        accuracy = (predictions == y_test).sum() / len(y_test)\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "# Create an instance of the NaiveBayesClassifier\n",
        "naive_bayes = NaiveBayesClassifier()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy on the test data\n",
        "accuracy = naive_bayes.score(X_test, y_test)\n",
        "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
        "\n",
        "# Print the instances and their predicted classes\n",
        "print(\"\\nInstances and Predictions:\")\n",
        "for instance, true_label in zip(X_test, y_test):\n",
        "    predicted_label = naive_bayes.predict([instance])[0]\n",
        "    print(f\"Instance: {instance} -> Predicted Class: {predicted_label}, True Class: {true_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrRU3xI-UR-J",
        "outputId": "ea15cbd9-6154-489c-d26e-9b815e3ea2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Dataset:\n",
            "  Weather Parents Money Decision (Category)\n",
            "0   Sunny     Yes  Rich              Cinema\n",
            "1   Sunny      No  Rich              Tennis\n",
            "2   Windy     Yes  Rich              Cinema\n",
            "3   Rainy     Yes  Poor              Cinema\n",
            "4   Rainy      No  RIch             Stay in\n",
            "Accuracy: 100.00%\n",
            "\n",
            "Instances and Predictions:\n",
            "Instance: ['Windy' 'Yes' 'Rich'] -> Predicted Class: Cinema, True Class: Cinema\n",
            "Instance: ['Sunny' 'No' 'Rich'] -> Predicted Class: Tennis, True Class: Tennis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "eps = np.finfo(float).eps\n",
        "from numpy import log2 as log\n",
        "\n",
        "def find_entropy(df):\n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    entropy = 0\n",
        "    values = df[Class].unique()\n",
        "    for value in values:\n",
        "        fraction = df[Class].value_counts()[value]/len(df[Class])\n",
        "        entropy += -fraction*np.log2(fraction)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def find_entropy_attribute(df,attribute):\n",
        "  Class = df.keys()[-1]\n",
        "  target_variables = df[Class].unique()  #This gives all 'Yes' and 'No'\n",
        "  variables = df[attribute].unique()\n",
        "    #This gives different features in that attribute (like 'Hot','Cold' in Temperature)\n",
        "  entropy2 = 0\n",
        "  for variable in variables:\n",
        "      entropy = 0\n",
        "      for target_variable in target_variables:\n",
        "          num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
        "          den = len(df[attribute][df[attribute]==variable])\n",
        "          fraction = num/(den+eps)\n",
        "          entropy += -fraction*log(fraction+eps)\n",
        "      fraction2 = den/len(df)\n",
        "      entropy2 += -fraction2*entropy\n",
        "  return abs(entropy2)\n",
        "\n",
        "\n",
        "def find_winner(df):\n",
        "    Entropy_att = []\n",
        "    IG = []\n",
        "    for key in df.keys()[:-1]:\n",
        "#         Entropy_att.append(find_entropy_attribute(df,key))\n",
        "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
        "    #rint(IG)\n",
        "    #print(df.keys()[:-1][np.argmax(IG)])\n",
        "    return df.keys()[:-1][np.argmax(IG)]\n",
        "\n",
        "\n",
        "def get_subtable(df, node,value):\n",
        "  return df[df[node] == value].reset_index(drop=True)\n",
        "\n",
        "\n",
        "def buildTree(df,tree=None):\n",
        "    Class = df.keys()[-1]\n",
        "    #Here we build our decision tree\n",
        "\n",
        "    #Get attribute with maximum information gain\n",
        "    node = find_winner(df)\n",
        "\n",
        "    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n",
        "    attValue = np.unique(df[node])\n",
        "\n",
        "    #Create an empty dictionary to create tree\n",
        "    if tree is None:\n",
        "        tree={}\n",
        "        tree[node] = {}\n",
        "\n",
        "   #We make loop to construct a tree by calling this function recursively.\n",
        "    #In this we check if the subset is pure and stops if it is pure.\n",
        "\n",
        "    for value in attValue:\n",
        "\n",
        "        subtable = get_subtable(df,node,value)\n",
        "        clValue,counts = np.unique(subtable['Decision (Category)'],return_counts=True)\n",
        "\n",
        "        if len(counts)==1:#Checking purity of subset\n",
        "            tree[node][value] = clValue[0]\n",
        "        else:\n",
        "            tree[node][value] = buildTree(subtable) #Calling the function recursively\n",
        "\n",
        "    return tree\n",
        "tree = buildTree(data)\n",
        "\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(tree)"
      ],
      "metadata": {
        "id": "hVLMopJWS1HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENDING ...........................\n"
      ],
      "metadata": {
        "id": "GC2B5S0OVSKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8th problem- A retail company may colledt the foloving information on houeeholds.\n",
        "Household income\n",
        "Household size\n",
        "Head of household Occupation\n",
        "Distance from nearest urban area\n",
        "Retail companies often use 10 identity groups of households that are similar to each other\n",
        "\n",
        "write a knn model for this ,without built in api ,plot the graph for the same with labels,generate your own dataset and export dataset to csv file\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTlieX2fLt6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8 th knn"
      ],
      "metadata": {
        "id": "7SH-Ks_RRYC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 200\n",
        "income = np.random.randint(10000, 100000, size=n_samples)\n",
        "size = np.random.randint(1, 6, size=n_samples)\n",
        "occupation = np.random.choice(['Engineer', 'Teacher', 'Doctor', 'Lawyer'], size=n_samples)\n",
        "distance = np.random.uniform(1, 100, size=n_samples)\n",
        "identity_group = np.random.randint(1, 11, size=n_samples)\n",
        "\n",
        "# Create a DataFrame from the generated data\n",
        "data = pd.DataFrame({\n",
        "    'HouseholdIncome': income,\n",
        "    'HouseholdSize': size,\n",
        "    'Occupation': occupation,\n",
        "    'DistanceFromUrbanArea': distance,\n",
        "    'IdentityGroup': identity_group\n",
        "})\n",
        "\n",
        "# Export dataset to a CSV file\n",
        "data.to_csv('household_data.csv', index=False)\n",
        "\n",
        "# Euclidean distance calculation\n",
        "def euclidean_distance(x1, x2):\n",
        "    distance = 0\n",
        "    for i in range(len(x1)):\n",
        "        if isinstance(x1[i], (int, float)):\n",
        "            distance += (x1[i] - x2[i]) ** 2\n",
        "    return np.sqrt(distance)\n",
        "\n",
        "# KNN model implementation\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # Compute distances between x and all examples in the training set\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        # Sort by distance and return indices of the first k neighbors\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        # Extract the labels of the k nearest neighbor training samples\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        # Return the most common class label\n",
        "        most_common = np.argmax(np.bincount(k_nearest_labels))\n",
        "        return most_common\n",
        "\n",
        "# Load dataset from CSV file\n",
        "dataset = pd.read_csv('household_data.csv')\n",
        "\n",
        "# Convert categorical variable to numerical using one-hot encoding\n",
        "dataset = pd.get_dummies(dataset, columns=['Occupation'])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop('IdentityGroup', axis=1).values\n",
        "y = dataset['IdentityGroup'].values\n",
        "\n",
        "# Instantiate and fit the KNN model\n",
        "knn = KNN(k=3)\n",
        "knn.fit(X, y)\n",
        "\n",
        "# Generate test data for visualization\n",
        "x1_test = np.random.randint(10000, 100000)\n",
        "x2_test = np.random.randint(1, 6)\n",
        "x3_test = np.random.uniform(1, 100)\n",
        "test_data = np.array([x1_test, x2_test, x3_test, 0, 0, 0, 0])  # Add zeros for one-hot encoded categorical variables\n",
        "\n",
        "# Predict the identity group for the test data\n",
        "predicted_group = knn.predict([test_data])\n",
        "\n",
        "# Plot all data points and color them based on the identity group\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, label in enumerate(np.unique(y)):\n",
        "    plt.scatter(\n",
        "        X[y == label, 0], X[y == label, 1],\n",
        "        label=f'Identity Group {label}',\n",
        "        alpha=0.7\n",
        "    )\n",
        "\n",
        "# Plot the test data point\n",
        "plt.scatter(test_data[0], test_data[1], color='red', marker='x', label='Test Data')\n",
        "\n",
        "plt.title('Household Identity Group Classification')\n",
        "plt.xlabel('Household Income')\n",
        "plt.ylabel('Household Size')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ccSIX2XKMBTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8th k means\n"
      ],
      "metadata": {
        "id": "F99mvzfORbMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 200\n",
        "income = np.random.randint(10000, 100000, size=n_samples)\n",
        "size = np.random.randint(1, 6, size=n_samples)\n",
        "occupation = np.random.choice(['Engineer', 'Teacher', 'Doctor', 'Lawyer'], size=n_samples)\n",
        "distance = np.random.uniform(1, 100, size=n_samples)\n",
        "identity_group = np.random.randint(1, 11, size=n_samples)\n",
        "\n",
        "# Create a DataFrame from the generated data\n",
        "data = pd.DataFrame({\n",
        "    'HouseholdIncome': income,\n",
        "    'HouseholdSize': size,\n",
        "    'Occupation': occupation,\n",
        "    'DistanceFromUrbanArea': distance,\n",
        "    'IdentityGroup': identity_group\n",
        "})\n",
        "\n",
        "# Export dataset to a CSV file\n",
        "data.to_csv('household_data.csv', index=False)\n",
        "\n",
        "# K-Means model implementation\n",
        "class KMeans:\n",
        "    def __init__(self, n_clusters):\n",
        "        self.n_clusters = n_clusters\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.centroids = X[np.random.choice(range(X.shape[0]), self.n_clusters, replace=False)]\n",
        "\n",
        "        while True:\n",
        "            # Assign each data point to the nearest centroid\n",
        "            labels = self._assign_labels(X)\n",
        "\n",
        "            # Update centroids based on the mean of the data points assigned to each centroid\n",
        "            new_centroids = self._update_centroids(X, labels)\n",
        "\n",
        "            # Check if the centroids have converged\n",
        "            if np.allclose(new_centroids, self.centroids):\n",
        "                break\n",
        "\n",
        "            self.centroids = new_centroids\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def _assign_labels(self, X):\n",
        "        distances = np.sqrt(((X[:, np.newaxis, :] - self.centroids) ** 2).sum(axis=2))\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = []\n",
        "        for i in range(self.n_clusters):\n",
        "            cluster_points = X[labels == i]\n",
        "            new_centroid = cluster_points.mean(axis=0)\n",
        "            new_centroids.append(new_centroid)\n",
        "        return np.array(new_centroids)\n",
        "\n",
        "# Load dataset from CSV file\n",
        "dataset = pd.read_csv('household_data.csv')\n",
        "\n",
        "# Preprocess the dataset (drop non-numeric columns)\n",
        "X = dataset.drop(['Occupation', 'IdentityGroup'], axis=1).values\n",
        "\n",
        "# Instantiate and fit the K-Means model\n",
        "kmeans = KMeans(n_clusters=10)\n",
        "labels = kmeans.fit(X)\n",
        "\n",
        "# Plot the graph with labels\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(1, 11):\n",
        "    plt.scatter(\n",
        "        X[labels == i, 0], X[labels == i, 1],\n",
        "        label=f'Identity Group {i}',\n",
        "        alpha=0.7\n",
        "    )\n",
        "\n",
        "plt.title('Household Clustering')\n",
        "plt.xlabel('Household Income')\n",
        "plt.ylabel('Household Size')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SgC423GLRc9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENDING ......................................................................\n"
      ],
      "metadata": {
        "id": "3RULpikYVIpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 ST PROGRAM LINEAR KNN\n",
        "\n"
      ],
      "metadata": {
        "id": "U7xfjh8rWH4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/sugar (1).csv\")\n",
        "\n",
        "# Select the feature (YearsExperience) and target (Salary) variables\n",
        "X = data[\"AGE X\"].values\n",
        "y = data[\"GLUCOSE LEVEL Y\"].values\n",
        "\n",
        "X_mean = np.mean(X)\n",
        "X_std = np.std(X)\n",
        "X = (X - X_mean) / X_std\n",
        "\n",
        "# Add a column of ones to X for the intercept term\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "# Initialize the coefficients to zeros\n",
        "theta = np.zeros(X.shape[1])\n",
        "\n",
        "# Set the learning rate and number of iterations\n",
        "alpha = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Loop over the specified number of iterations\n",
        "for i in range(num_iterations):\n",
        "    # Calculate the predicted values\n",
        "    y_pred = X.dot(theta)\n",
        "\n",
        "    # Calculate the error between the predicted values and the true values\n",
        "    error = y_pred - y\n",
        "\n",
        "    # Update the coefficients using the LMS algorithm\n",
        "    theta -= alpha * X.T.dot(error) / X.shape[0]\n",
        "\n",
        "# Print the coefficients\n",
        "print(\"Coefficients:\", theta)\n",
        "\n",
        "# Calculate R-squared\n",
        "y_mean = np.mean(y)\n",
        "ss_total = np.sum((y - y_mean) ** 2)\n",
        "ss_residual = np.sum((y - y_pred) ** 2)\n",
        "r_squared = 1 - (ss_residual / ss_total)\n",
        "print(\"R-squared:\", r_squared)\n",
        "\n",
        "# Calculate accuracy in percentage\n",
        "accuracy = r_squared * 100\n",
        "print(\"Accuracy:\", accuracy, \"%\")\n",
        "\n",
        "# Plot the data points and the line of best fit\n",
        "plt.scatter(X[:, 1], y, alpha=0.7)\n",
        "plt.plot(X[:, 1], X.dot(theta), color='red')\n",
        "plt.xlabel(\"AGE X\")\n",
        "plt.ylabel(\"GLUCOSE LEVEL Y\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "JThQ-xSNWHlZ",
        "outputId": "f1848ef2-d15f-4d07-d781-93859abf83ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [80.99650313  5.53957946]\n",
            "R-squared: 0.28069735787500116\n",
            "Accuracy: 28.069735787500115 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7th Bayesian"
      ],
      "metadata": {
        "id": "Om-bvgnGaCoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the dataset\n",
        "dataset = np.array([\n",
        "    ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'],\n",
        "    ['Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'],\n",
        "    ['No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No'],\n",
        "    ['No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No'],\n",
        "    ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No'],\n",
        "    ['Yes', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No'],\n",
        "    ['No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No'],\n",
        "    ['No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'],\n",
        "    ['Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No'],\n",
        "    ['No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No'],\n",
        "    ['Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes'],\n",
        "])\n",
        "\n",
        "# Define the variables\n",
        "variables = ['Creative', 'Smart', 'Party', 'Project', 'Mac', 'HW', 'Success', 'Happy']\n",
        "\n",
        "# Calculate the prior probabilities\n",
        "prior_probabilities = {}\n",
        "for value in ['Yes', 'No']:\n",
        "    prior_probabilities[value] = np.count_nonzero(dataset[:, -1] == value) / len(dataset)\n",
        "\n",
        "# Calculate the conditional probabilities\n",
        "conditional_probabilities = {}\n",
        "for variable in variables[:-1]:\n",
        "    conditional_probabilities[variable] = {}\n",
        "    for value in ['Yes', 'No']:\n",
        "        conditional_probabilities[variable][value] = {}\n",
        "        for outcome in ['Yes', 'No']:\n",
        "            count = np.count_nonzero((dataset[:, variables.index(variable)] == value) & (dataset[:, -1] == outcome))\n",
        "            total = np.count_nonzero(dataset[:, variables.index(variable)] == value)\n",
        "            conditional_probabilities[variable][value][outcome] = count / total\n",
        "\n",
        "# Define a function to predict the value of 'Happy' for a given instance\n",
        "def predict(instance, prior_probabilities, conditional_probabilities):\n",
        "    # Initialize the predicted value for 'Happy'\n",
        "    predicted_value = None\n",
        "    max_posterior = -1\n",
        "\n",
        "    # Iterate over possible outcomes ('Yes', 'No')\n",
        "    for outcome in ['Yes', 'No']:\n",
        "        # Calculate the posterior probability for the outcome\n",
        "        posterior = prior_probabilities[outcome]\n",
        "\n",
        "        # Iterate over variables\n",
        "        for variable in variables[:-1]:\n",
        "            value = instance[variables.index(variable)]\n",
        "            if value in conditional_probabilities[variable]:\n",
        "                posterior *= conditional_probabilities[variable][value][outcome]\n",
        "\n",
        "        # Update the predicted value if the posterior is higher\n",
        "        if posterior > max_posterior:\n",
        "            predicted_value = outcome\n",
        "            max_posterior = posterior\n",
        "\n",
        "    return predicted_value\n",
        "\n",
        "# Test the Naive Bayes classifier on the dataset\n",
        "predictions = []\n",
        "for instance in dataset:\n",
        "    prediction = predict(instance, prior_probabilities, conditional_probabilities)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Print the predictions\n",
        "print('Predictions:', predictions)\n",
        "\n",
        "# Calculate the accuracy\n",
        "def calculate_accuracy(predictions, actual_values):\n",
        "    correct = np.count_nonzero(predictions == actual_values)\n",
        "    total = len(actual_values)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Define the actual values from the dataset\n",
        "actual_values = dataset[:, -1]\n",
        "\n",
        "# Calculate the accuracy of the Naive Bayes classifier\n",
        "accuracy = calculate_accuracy(np.array(predictions), actual_values)\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3Fgps0JaJFm",
        "outputId": "1e142a67-66a3-4926-8109-1accd95bd49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n",
            "Accuracy: 0.8181818181818182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Naive Bayes"
      ],
      "metadata": {
        "id": "aFSYjCF2awBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the dataset\n",
        "dataset = np.array([\n",
        "    ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'],\n",
        "    ['Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'],\n",
        "    ['No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No'],\n",
        "    ['No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No'],\n",
        "    ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No'],\n",
        "    ['Yes', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No'],\n",
        "    ['No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No'],\n",
        "    ['No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'],\n",
        "    ['Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No'],\n",
        "    ['No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No'],\n",
        "    ['Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes'],\n",
        "])\n",
        "\n",
        "\n",
        "# Define the variables\n",
        "variables = ['Creative', 'Smart', 'Party', 'Project', 'Mac', 'HW', 'Success', 'Happy']\n",
        "\n",
        "# Calculate the prior probabilities\n",
        "prior_probabilities = {}\n",
        "for value in ['Yes', 'No']:\n",
        "    prior_probabilities[value] = np.count_nonzero(dataset[:, -1] == value) / len(dataset)\n",
        "\n",
        "# Define the conditional probability tables (CPTs)\n",
        "cpt = {}\n",
        "for variable in variables:\n",
        "    cpt[variable] = {}\n",
        "\n",
        "# Calculate the conditional probabilities\n",
        "for variable in variables[:-1]:\n",
        "    if variable == 'Project':\n",
        "        for value in ['Yes', 'No']:\n",
        "            cpt[variable][value] = {}\n",
        "            for outcome in ['Yes', 'No']:\n",
        "                count = np.count_nonzero((dataset[:, variables.index(variable)] == value) & (dataset[:, -1] == outcome))\n",
        "                total = np.count_nonzero(dataset[:, variables.index(variable)] == value)\n",
        "                if total == 0:\n",
        "                    cpt[variable][value][outcome] = 1e-6  # Assign a small default probability\n",
        "                else:\n",
        "                    cpt[variable][value][outcome] = count / total\n",
        "    elif variable == 'Mac':\n",
        "        for value in ['Yes', 'No']:\n",
        "            cpt[variable][value] = {}\n",
        "            for outcome in ['Yes', 'No']:\n",
        "                count = np.count_nonzero((dataset[:, variables.index(variable)] == value) & (dataset[:, -1] == outcome))\n",
        "                total = np.count_nonzero(dataset[:, variables.index(variable)] == value)\n",
        "                if total == 0:\n",
        "                    cpt[variable][value][outcome] = 1e-6  # Assign a small default probability\n",
        "                else:\n",
        "                    cpt[variable][value][outcome] = count / total\n",
        "    elif variable == 'HW':\n",
        "        for value in ['Yes', 'No']:\n",
        "            cpt[variable][value] = {}\n",
        "            for outcome in ['Yes', 'No']:\n",
        "                count = np.count_nonzero((dataset[:, variables.index(variable)] == value) & (dataset[:, -1] == outcome))\n",
        "                total = np.count_nonzero(dataset[:, variables.index(variable)] == value)\n",
        "                if total == 0:\n",
        "                    cpt[variable][value][outcome] = 1e-6  # Assign a small default probability\n",
        "                else:\n",
        "                    cpt[variable][value][outcome] = count / total\n",
        "    elif variable == 'Success':\n",
        "        for value in ['Yes', 'No']:\n",
        "            cpt[variable][value] = {}\n",
        "            for outcome in ['Yes', 'No']:\n",
        "                count = np.count_nonzero((dataset[:, variables.index(variable)] == value) & (dataset[:, -1] == outcome))\n",
        "                total = np.count_nonzero(dataset[:, variables.index(variable)] == value)\n",
        "                if total == 0:\n",
        "                    cpt[variable][value][outcome] = 1e-6  # Assign a small default probability\n",
        "                else:\n",
        "                    cpt[variable][value][outcome] = count / total\n",
        "    else:\n",
        "        for value in ['Yes', 'No']:\n",
        "            cpt[variable][value] = {}\n",
        "            for outcome in ['Yes', 'No']:\n",
        "                count = np.count_nonzero((dataset[:, variables.index(variable)] == value) & (dataset[:, -1] == outcome))\n",
        "                total = np.count_nonzero(dataset[:, variables.index(variable)] == value)\n",
        "                if total == 0:\n",
        "                    cpt[variable][value][outcome] = 1e-6  # Assign a small default probability\n",
        "                else:\n",
        "                    cpt[variable][value][outcome] = count / total\n",
        "\n",
        "# Define a function to predict the value of 'Happy' for a given instance\n",
        "def predict(instance, prior_probabilities, cpt):\n",
        "    # Initialize the predicted value for 'Happy'\n",
        "    predicted_value = None\n",
        "    max_posterior = -1\n",
        "\n",
        "    # Iterate over possible outcomes ('Yes', 'No')\n",
        "    for outcome in ['Yes', 'No']:\n",
        "        # Calculate the posterior probability for the outcome\n",
        "        posterior = prior_probabilities[outcome]\n",
        "\n",
        "        # Calculate the conditional probability based on dependencies\n",
        "        if instance[variables.index('Project')] == 'Yes':\n",
        "            posterior *= cpt['Project']['Yes'][outcome]\n",
        "        if instance[variables.index('Mac')] == 'Yes':\n",
        "            posterior *= cpt['Mac']['Yes'][outcome]\n",
        "        if instance[variables.index('HW')] == 'Yes':\n",
        "            posterior *= cpt['HW']['Yes'][outcome]\n",
        "        if instance[variables.index('Success')] == 'Yes':\n",
        "            posterior *= cpt['Success']['Yes'][outcome]\n",
        "\n",
        "        # Update the predicted value if the posterior is higher\n",
        "        if posterior > max_posterior:\n",
        "            predicted_value = outcome\n",
        "            max_posterior = posterior\n",
        "\n",
        "    return predicted_value\n",
        "\n",
        "# Test the Bayesian network on the dataset\n",
        "predictions = []\n",
        "for instance in dataset:\n",
        "    prediction = predict(instance, prior_probabilities, cpt)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Print the predictions\n",
        "print('Predictions:', predictions)\n",
        "\n",
        "# Calculate the accuracy\n",
        "def calculate_accuracy(predictions, actual_values):\n",
        "    correct = np.count_nonzero(predictions == actual_values)\n",
        "    total = len(actual_values)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Define the actual values from the dataset\n",
        "actual_values = dataset[:, -1]\n",
        "\n",
        "# Calculate the accuracy of the Bayesian network\n",
        "accuracy = calculate_accuracy(np.array(predictions), actual_values)\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRJ63_w2aO1q",
        "outputId": "c5fe2900-10d5-43b1-a954-328190bc6d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes']\n",
            "Accuracy: 0.6363636363636364\n"
          ]
        }
      ]
    }
  ]
}